<?xml version='1.0' encoding='utf-8'?>

<chapter xml:id="ch-distributions" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Distributions</title>

  <introduction>
    <p>
      In the previous chapter we used Bayes's Theorem to solve a cookie problem; then we solved it again using a Bayes table.
      In this chapter, at the risk of testing your patience, we will solve it one more time using a <c>Pmf</c> object, which represents a <q>probability mass function</q>.
      I'll explain what that means, and why it is useful for Bayesian statistics.
    </p>
    <p>
      We'll use <c>Pmf</c> objects to solve some more challenging problems and take one more step toward Bayesian statistics.
      But we'll start with distributions.
    </p>
  </introduction>

  <section xml:id="sec-distributions-distributions">
    <title>Distributions</title>

    <p>
      In statistics a <term>distribution</term> is a set of possible outcomes and their corresponding probabilities.
      For example, if you toss a coin, there are two possible outcomes with approximately equal probability.
      If you roll a six-sided die, the set of possible outcomes is the numbers 1 to 6, and the probability associated with each outcome is 1/6.
    </p>

    <p>
      To represent distributions, we'll use a library called <c>empiricaldist</c>.
      An <q>empirical</q> distribution is based on data, as opposed to a theoretical distribution.
      We'll use this library throughout the book. I'll introduce the basic features in this chapter and we'll see additional features later.
    </p>
  </section>

  <section xml:id="sec-distributions-pmf">
    <title>Probability Mass Functions</title>

    <p>
      If the outcomes in a distribution are discrete, we can describe the distribution with a <term>probability mass function</term>, or PMF, which is a function that maps from each possible outcome to its probability.
    </p>

    <p>
      <c>empiricaldist</c> provides a class called <c>Pmf</c> that represents a probability mass function.
      To use <c>Pmf</c> you can import it like this:
    </p>

    <listing xml:id="list-import-pmf">
      <program language="python">
        <input>
from empiricaldist import Pmf
        </input>
      </program>
    </listing>

    <p>
      The following example makes a <c>Pmf</c> that represents the outcome of a coin toss.
    </p>

      <program language="python">
        <input>
coin = Pmf()
coin['heads'] = 1/2
coin['tails'] = 1/2
coin
        </input>
      
      <output>
      heads    0.5
tails    0.5
dtype: float64
      </output>
    </program>
<table xml:id="tbl-coin-probs">
  <title>Probability distribution for a fair coin.</title>
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>

    <row>
      <cell>heads</cell>
      <cell>0.5</cell>
    </row>

    <row>
      <cell>tails</cell>
      <cell>0.5</cell>
    </row>
  </tabular>
</table>

    <p>
      <c>Pmf()</c> creates an empty <c>Pmf</c> with no outcomes.
      Then we can add new outcomes using the bracket operator.
      In this example, the two outcomes are represented with strings, and they have the same probability, 0.5.
    </p>

    <p>
      You can also make a <c>Pmf</c> from a sequence of possible outcomes.
      The following example uses <c>Pmf.from_seq</c> to make a <c>Pmf</c> that represents a six-sided die.
    </p>

      <program language="python">
        <input>
die = Pmf.from_seq([1,2,3,4,5,6])
die
        </input>
      
      <output>
      1    0.166667
2    0.166667
3    0.166667
4    0.166667
5    0.166667
6    0.166667
Name: , dtype: float64
      </output>
    </program>
<table xml:id="tbl-die-throw">
  <title>Probability distribution for a fair 6-sided die.</title>
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>

    <row>
      <cell>1</cell>
      <cell>0.166667</cell>
    </row>

    <row>
      <cell>2</cell>
      <cell>0.166667</cell>
    </row>
    <row>
      <cell>3</cell>
      <cell>0.166667</cell>
    </row>
    <row>
      <cell>4</cell>
      <cell>0.166667</cell>
    </row>
    <row>
      <cell>5</cell>
      <cell>0.166667</cell>
    </row>
    <row>
      <cell>6</cell>
      <cell>0.166667</cell>
    </row>
  </tabular>
</table>
    <p>
      In this example, all outcomes in the sequence appear once, so they all have the same probability, <m>1/6</m>.
      More generally, outcomes can appear more than once, as in the following example:
    </p>

      <program language="python">
        <input>
letters = Pmf.from_seq(list('Mississippi'))
letters
        </input>
      
      <output>
      M    0.090909
i    0.363636
p    0.181818
s    0.363636
Name: , dtype: float64
      </output>
    </program>

<table xml:id="tbl-mississippi">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>

    <row>
      <cell>M</cell>
      <cell>0.090909</cell>
    </row>

    <row>
      <cell>i</cell>
      <cell>0.363636</cell>
    </row>
    <row>
      <cell>p</cell>
      <cell>0.181818</cell>
    </row>
    <row>
      <cell>s</cell>
      <cell>0.363636</cell>
    </row>
  </tabular>
</table>
    <p>
      The letter <c>M</c> appears once out of 11 characters, so its probability is <m>1/11</m>.
      The letter <c>i</c> appears 4 times, so its probability is <m>4/11</m>.
    </p>

    <p>
      Since the letters in a string are not outcomes of a random process, I'll use the more general term <q>quantities</q> for the letters in the <c>Pmf</c>.
    </p>

    <p>
      The <c>Pmf</c> class inherits from a Pandas <c>Series</c>, so anything you can do with a <c>Series</c>, you can also do with a <c>Pmf</c>.
      For example, you can use the bracket operator to look up a quantity and get the corresponding probability.
    </p>

      <program language="python">
        <input>
letters['s']
        </input>
      
      <output>
      0.36363636363636365
      </output>
    </program>
    <p>
      In the word <q>Mississippi</q>, about 36% of the letters are <q>s</q>.
    </p>

    <p>
      However, if you ask for the probability of a quantity that's not in the distribution, you get a <c>KeyError</c>.
      You can also call a <c>Pmf</c> as if it were a function, with a letter in parentheses.
    </p>

      <program language="python">
        <input>
letters('s')
        </input>
      
      <output>
      0.36363636363636365
      </output>
    </program>
    <p>
      If the quantity is in the distribution the results are the same.
      But if it is not in the distribution, the result is <c>0</c>, not an error.
    </p>

    <listing xml:id="list-lookup-missing">
      <program language="python">
        <input>
letters('t')
        </input>
      
      <output>
      0
      </output>
    </program>
    </listing>
    <p>
      With parentheses, you can also provide a sequence of quantities and get a sequence of probabilities.
    </p>

      <program language="python">
        <input>
die([1,4,7])
        </input>
      
      <output>
      array([0.16666667, 0.16666667, 0.        ])
      </output>
    </program>

    <p>
      The quantities in a <c>Pmf</c> can be strings, numbers, or any other type that can be stored in the index of a Pandas <c>Series</c>.
      If you are familiar with Pandas, that will help you work with <c>Pmf</c> objects.
      But I will explain what you need to know as we go along.
    </p>
  </section>

  <section xml:id="sec-distributions-cookie-revisited">
    <title>The Cookie Problem Revisited</title>

    <p>
      In this section I'll use a <c>Pmf</c> to solve the cookie problem from <xref ref="ch-bayes-theorem" />.
      Here's the statement of the problem again:
    </p>

    <blockquote>
      <p>
        Suppose there are two bowls of cookies.
      </p>
      <p>
        <ul>
          <li><p>Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies.</p></li>
          <li><p>Bowl 2 contains 20 vanilla cookies and 20 chocolate cookies.</p></li>
        </ul>
      </p>
      <p>
        Now suppose you choose one of the bowls at random and, without looking, choose a cookie at random.
        If the cookie is vanilla, what is the probability that it came from Bowl 1?
      </p>
    </blockquote>

    <p>
      Here's a <c>Pmf</c> that represents the two hypotheses and their prior probabilities:
    </p>

      <program language="python">
        <input>
prior = Pmf.from_seq(['Bowl 1', 'Bowl 2'])
prior
        </input>
      
      <output>
      Bowl 1    0.5
Bowl 2    0.5
Name: , dtype: float64
      </output>
    </program>
<table xml:id="tbl-bowl-probs-1">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>

    <row>
      <cell>Bowl 1</cell>
      <cell>0.5</cell>
    </row>

    <row>
      <cell>Bowl 2</cell>
      <cell>0.5</cell>
    </row>
  </tabular>
</table>
    <p>
      This distribution, which contains the prior probability for each hypothesis, is called (wait for it) the <term>prior distribution</term>.
    </p>

    <p>
      To update the distribution based on new data (the vanilla cookie), we multiply the priors by the likelihoods.
      The likelihood of drawing a vanilla cookie from Bowl 1 is <m>3/4</m>.
      The likelihood for Bowl 2 is <m>1/2</m>.
    </p>

      <program language="python">
        <input>
likelihood_vanilla = [0.75, 0.5]
posterior = prior * likelihood_vanilla
posterior
        </input>
      </program>
<table xml:id="tbl-bowl-probs-2">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>

    <row>
      <cell>Bowl 1</cell>
      <cell>0.375</cell>
    </row>

    <row>
      <cell>Bowl 2</cell>
      <cell>0.250</cell>
    </row>
  </tabular>
</table>
    <p>
      The result is the unnormalized posteriors; that is, they don’t add up to 1. To make them add up to 1, we can use <c>normalize</c>, which is a method provided by <c>Pmf</c>
    </p>

      <program language="python">
        <input>
posterior.normalize()
        </input>
      </program>
    <pre>
0.625
</pre>
    <p>
     The return value from <c>normalize</c> is the total probability of the data, which is <m>5/8</m>.
     <c>posterior</c>, which contains the posterior probability for each hypothesis, is called (wait now) the <term>posterior distribution</term>.
    </p>

      <program language="python">
        <input>
posterior
        </input>
      </program>
<table xml:id="tbl-bowl-probs-2b">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>

    <row>
      <cell>Bowl 1</cell>
      <cell>0.6</cell>
    </row>

    <row>
      <cell>Bowl 2</cell>
      <cell>0.4</cell>
    </row>
  </tabular>
</table>
    <p>
      From the posterior distribution we can select the posterior probability for Bowl 1:
    </p>

      <program language="python">
        <input>
posterior('Bowl 1')
        </input>
      
      <output>
      0.6
      </output>
    </program>
    <p>
And the answer is 0.6.
</p>
<p>One benefit of using <c>Pmf</c> objects is that it is easy to do successive updates with more data. For example, suppose you put the first cookie back (so the contents of the bowls don’t change) and draw again from the same bowl. If the second cookie is also vanilla, we can do a second update like this:    </p>

      <program language="python">
        <input>
posterior *= likelihood_vanilla
posterior.normalize()
posterior
</input>
      
      <output>
      Bowl 1    0.692308
Bowl 2    0.307692
Name: , dtype: float64
      </output>
    </program>
<table xml:id="tbl-bowl-probs-3">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>

    <row>
      <cell>Bowl 1</cell>
      <cell>0.692308</cell>
    </row>

    <row>
      <cell>Bowl 2</cell>
      <cell>0.307692</cell>
    </row>
  </tabular>
</table>
    <p>
Now the posterior probability for Bowl 1 is almost 70%. But suppose we do the same thing again and get a chocolate cookie.
</p><p>
Here are the likelihoods for the new data:
</p>

      <program language="python">
        <input>
likelihood_chocolate = [0.25, 0.5]
        </input>
      </program>

    <p>
And here's the update
</p>
      <program language="python">
        <input>
posterior *= likelihood_chocolate
posterior.normalize()
posterior
</input>      
      <output>
      Bowl 1    0.529412
Bowl 2    0.470588
Name: , dtype: float64
      </output>
    </program>
<table xml:id="tbl-bowl-probs-4">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>

    <row>
      <cell>Bowl 1</cell>
      <cell>0.529412</cell>
    </row>

    <row>
      <cell>Bowl 2</cell>
      <cell>0.470588</cell>
    </row>
  </tabular>
</table>
<p>
Now the posterior probability for Bowl 1 is about 53%. After two vanilla cookies and one chocolate, the posterior probabilities are close to 50/50.
</p>
  </section>

  <section xml:id="sec-distributions-101-bowls">
    <title>101 Bowls</title>

    <p>
      Next let's solve a cookie problem with 101 bowls:
    </p>

    <p>
      <ul>
        <li><p>Bowl 0 contains 0% vanilla cookies,</p></li>
        <li><p>Bowl 1 contains 1% vanilla cookies,</p></li>
        <li><p>Bowl 2 contains 2% vanilla cookies,</p></li>
      </ul>
    </p>

    <p>
      and so on, up to
    </p>

    <p>
      <ul>
        <li><p>Bowl 99 contains 99% vanilla cookies, and</p></li>
        <li><p>Bowl 100 contains all vanilla cookies.</p></li>
      </ul>
    </p>

    <p>
      As in the previous version, there are only two kinds of cookies, vanilla and chocolate.
      So Bowl 0 is all chocolate cookies, Bowl 1 is 99% chocolate, and so on.
      Suppose we choose a bowl at random, choose a cookie at random, and it turns out to be vanilla.
      What is the probability that the cookie came from Bowl <m>x</m>, for each value of <m>x</m>?
    </p>

    <p>
      To solve this problem, I'll use <c>np.arange</c> to make an array that represents 101 hypotheses, numbered from 0 to 100.
    </p>

    <listing xml:id="list-101-hypos">
      <program language="python">
        <input>
import numpy as np

hypos = np.arange(101)
        </input>
      </program>
    </listing>

    <p>
      We can use this array to make the prior distribution:
    </p>

    <listing xml:id="list-101-prior">
      <program language="python">
        <input>
prior = Pmf(1, hypos)
prior.normalize()
        </input>
      </program>
    </listing>
<pre>
101
</pre>

    <p>
      As this example shows, we can initialize a <c>Pmf</c> with two parameters.
      The first parameter is the prior probability; the second parameter is a sequence of quantities.
    </p>

    <p>
      In this example, the probabilities are all the same, so we only have to provide one of them; it gets <q>broadcast</q> across the hypotheses.
      Since all hypotheses have the same prior probability, this distribution is <term>uniform</term>.
    </p>

    <p>
      Here are the first few hypotheses and their probabilities.
    </p>

    <listing xml:id="list-101-head">
      <program language="python">
        <input>
prior.head()
        </input>
      </program>
    </listing>
<table xml:id="tbl-101-prior-head">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>
    <row>
      <cell>0</cell>
      <cell>0.009901</cell>
    </row>
    <row>
      <cell>1</cell>
      <cell>0.009901</cell>
    </row>
    <row>
      <cell>2</cell>
      <cell>0.009901</cell>
    </row>
  </tabular>
</table>

    <p>
      The likelihood of the data is the fraction of vanilla cookies in each bowl, which we can calculate using <c>hypos</c>:
    </p>

    <listing xml:id="list-101-likelihood">
      <program language="python">
        <input>
likelihood_vanilla = hypos/100
likelihood_vanilla[:5]
        </input>
      </program>
    </listing>
<pre>
array([0.  , 0.01, 0.02, 0.03, 0.04])
</pre>

    <p>
      Now we can compute the posterior distribution in the usual way:
    </p>

    <listing xml:id="list-101-posterior1">
      <program language="python">
        <input>
posterior1 = prior * likelihood_vanilla
posterior1.normalize()
posterior1.head()
        </input>
      </program>
    </listing>
<table xml:id="tbl-101-posterior1-head">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>
    <row>
      <cell>0</cell>
      <cell>0.000000</cell>
    </row>
    <row>
      <cell>1</cell>
      <cell>0.000198</cell>
    </row>
    <row>
      <cell>2</cell>
      <cell>0.000396</cell>
    </row>
  </tabular>
</table>

    <p>
      The following figure shows the prior distribution and the posterior distribution after one vanilla cookie.
    </p>

    <listing xml:id="list-101-plot1">
      <program language="python">
        <input>
prior.plot(label='prior', color='C5')
posterior1.plot(label='posterior', color='C4')
plt.xlabel('Bowl #')
plt.ylabel('PMF')
plt.title('Posterior after one vanilla cookie')
plt.legend()
        </input>
      </program>
    </listing>

    <p>
      The posterior probability of Bowl 0 is 0 because it contains no vanilla cookies.
      The posterior probability of Bowl 100 is the highest because it contains the most vanilla cookies.
      In between, the shape of the posterior distribution is a line because the likelihoods are proportional to the bowl numbers.
    </p>

    <p>
      Now suppose we put the cookie back, draw again from the same bowl, and get another vanilla cookie.
      Here's the update after the second cookie:
    </p>

    <listing xml:id="list-101-posterior2">
      <program language="python">
        <input>
posterior2 = posterior1 * likelihood_vanilla
posterior2.normalize()
        </input>
      </program>
    </listing>
<pre>
0.67
</pre>

    <p>
      And here's what the posterior distribution looks like.
    </p>

    <listing xml:id="list-101-plot2">
      <program language="python">
        <input>
posterior2.plot(label='posterior', color='C4')
plt.xlabel('Bowl #')
plt.ylabel('PMF')
plt.title('Posterior after two vanilla cookies')
        </input>
      </program>
    </listing>

    <p>
      After two vanilla cookies, the high-numbered bowls have the highest posterior probabilities because they contain the most vanilla cookies; the low-numbered bowls have the lowest probabilities.
    </p>

    <p>
      But suppose we draw again and get a chocolate cookie.
      Here's the update:
    </p>

    <listing xml:id="list-101-posterior3">
      <program language="python">
        <input>
likelihood_chocolate = 1 - hypos/100

posterior3 = posterior2 * likelihood_chocolate
posterior3.normalize()
        </input>
      </program>
    </listing>
<pre>
0.2462686567164179
</pre>

    <p>
      And here's the posterior distribution.
    </p>

    <listing xml:id="list-101-plot3">
      <program language="python">
        <input>
posterior3.plot(label='posterior', color='C4')
plt.xlabel('Bowl #')
plt.ylabel('PMF')
plt.title('Posterior after 2 vanilla, 1 chocolate')
        </input>
      </program>
    </listing>

    <p>
      Now Bowl 100 has been eliminated because it contains no chocolate cookies.
      But the high-numbered bowls are still more likely than the low-numbered bowls, because we have seen more vanilla cookies than chocolate.
    </p>

    <p>
      In fact, the peak of the posterior distribution is at Bowl 67, which corresponds to the fraction of vanilla cookies in the data we've observed, <m>2/3</m>.
    </p>

    <p>
      The quantity with the highest posterior probability is called the <term>MAP</term>, which stands for <q>maximum a posteriori probability</q>, where <q>a posteriori</q> is unnecessary Latin for <q>posterior</q>.
    </p>

    <p>
      To compute the MAP, we can use the <c>Series</c> method <c>idxmax</c>:
    </p>

    <listing xml:id="list-101-map">
      <program language="python">
        <input>
posterior3.idxmax()
        </input>
      </program>
    </listing>
<pre>
67
</pre>

    <p>
      Or <c>Pmf</c> provides a more memorable name for the same thing:
    </p>

    <listing xml:id="list-101-max-prob">
      <program language="python">
        <input>
posterior3.max_prob()
        </input>
      </program>
    </listing>
<pre>
67
</pre>

    <p>
      As you might suspect, this example isn't really about bowls; it's about estimating proportions.
      Imagine that you have one bowl of cookies.
      You don't know what fraction of cookies are vanilla, but you think it is equally likely to be any fraction from 0 to 1.
      If you draw three cookies and two are vanilla, what proportion of cookies in the bowl do you think are vanilla?
      The posterior distribution we just computed is the answer to that question.
    </p>

    <p>
      We'll come back to estimating proportions in the next chapter.
      But first let's use a <c>Pmf</c> to solve the dice problem.
    </p>
  </section>

  <section xml:id="sec-distributions-dice">
    <title>The Dice Problem</title>

    <p>
      In the previous chapter we solved the dice problem using a Bayes table.
      Here's the statement of the problem:
    </p>

    <blockquote>
      <p>
        Suppose I have a box with a 6-sided die, an 8-sided die, and a 12-sided die.
        I choose one of the dice at random, roll it, and report that the outcome is a 1.
        What is the probability that I chose the 6-sided die?
      </p>
    </blockquote>

    <p>
      Let's solve it using a <c>Pmf</c>.
      I'll use integers to represent the hypotheses:
    </p>

    <listing xml:id="list-dice-hypos">
      <program language="python">
        <input>
hypos = [6, 8, 12]
        </input>
      </program>
    </listing>

    <p>
      We can make the prior distribution like this:
    </p>

    <listing xml:id="list-dice-prior">
      <program language="python">
        <input>
prior = Pmf(1/3, hypos)
prior
        </input>
      </program>
    </listing>
<table xml:id="tbl-dice-prior">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>
    <row>
      <cell>6</cell>
      <cell>0.333333</cell>
    </row>
    <row>
      <cell>8</cell>
      <cell>0.333333</cell>
    </row>
    <row>
      <cell>12</cell>
      <cell>0.333333</cell>
    </row>
  </tabular>
</table>

    <p>
      As in the previous example, the prior probability gets broadcast across the hypotheses.
      The <c>Pmf</c> object has two attributes:
    </p>

    <p>
      <ul>
        <li><p><c>qs</c> contains the quantities in the distribution;</p></li>
        <li><p><c>ps</c> contains the corresponding probabilities.</p></li>
      </ul>
    </p>

    <listing xml:id="list-dice-qs">
      <program language="python">
        <input>
prior.qs
        </input>
      </program>
    </listing>
<pre>
array([ 6,  8, 12])
</pre>

    <listing xml:id="list-dice-ps">
      <program language="python">
        <input>
prior.ps
        </input>
      </program>
    </listing>
<pre>
array([0.33333333, 0.33333333, 0.33333333])
</pre>

    <p>
      Now we're ready to do the update.
      Here's the likelihood of the data for each hypothesis.
    </p>

    <listing xml:id="list-dice-likelihood1">
      <program language="python">
        <input>
likelihood1 = 1/6, 1/8, 1/12
        </input>
      </program>
    </listing>

    <p>
      And here's the update.
    </p>

    <listing xml:id="list-dice-posterior1">
      <program language="python">
        <input>
posterior = prior * likelihood1
posterior.normalize()
posterior
        </input>
      </program>
    </listing>
<table xml:id="tbl-dice-posterior1">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>
    <row>
      <cell>6</cell>
      <cell>0.444444</cell>
    </row>
    <row>
      <cell>8</cell>
      <cell>0.333333</cell>
    </row>
    <row>
      <cell>12</cell>
      <cell>0.222222</cell>
    </row>
  </tabular>
</table>

    <p>
      The posterior probability for the 6-sided die is <m>4/9</m>.
    </p>

    <p>
      Now suppose I roll the same die again and get a 7.
      Here are the likelihoods:
    </p>

    <listing xml:id="list-dice-likelihood2">
      <program language="python">
        <input>
likelihood2 = 0, 1/8, 1/12
        </input>
      </program>
    </listing>

    <p>
      The likelihood for the 6-sided die is 0 because it is not possible to get a 7 on a 6-sided die.
      The other two likelihoods are the same as in the previous update.
    </p>

    <p>
      Here's the update:
    </p>

    <listing xml:id="list-dice-posterior2">
      <program language="python">
        <input>
posterior *= likelihood2
posterior.normalize()
posterior
        </input>
      </program>
    </listing>
<table xml:id="tbl-dice-posterior2">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>
    <row>
      <cell>6</cell>
      <cell>0.000000</cell>
    </row>
    <row>
      <cell>8</cell>
      <cell>0.692308</cell>
    </row>
    <row>
      <cell>12</cell>
      <cell>0.307692</cell>
    </row>
  </tabular>
</table>

    <p>
      After rolling a 1 and a 7, the posterior probability of the 8-sided die is about 69%.
    </p>
  </section>

  <section xml:id="sec-distributions-updating-dice">
    <title>Updating Dice</title>

    <p>
      The following function is a more general version of the update in the previous section:
    </p>

    <listing xml:id="list-update-dice-function">
      <program language="python">
        <input>
def update_dice(pmf, data):
    """Update pmf based on new data."""
    hypos = pmf.qs
    likelihood = 1 / hypos
    impossible = (data > hypos)
    likelihood[impossible] = 0
    pmf *= likelihood
    pmf.normalize()
        </input>
      </program>
    </listing>

    <p>
      The first parameter is a <c>Pmf</c> that represents the possible dice and their probabilities.
      The second parameter is the outcome of rolling a die.
    </p>

    <p>
      The first line selects quantities from the <c>Pmf</c> which represent the hypotheses.
      Since the hypotheses are integers, we can use them to compute the likelihoods.
      In general, if there are <c>n</c> sides on the die, the probability of any possible outcome is <c>1/n</c>.
    </p>

    <p>
      However, we have to check for impossible outcomes!
      If the outcome exceeds the hypothetical number of sides on the die, the probability of that outcome is 0.
    </p>

    <p>
      <c>impossible</c> is a Boolean <c>Series</c> that is <c>True</c> for each impossible outcome.
      I use it as an index into <c>likelihood</c> to set the corresponding probabilities to 0.
    </p>

    <p>
      Finally, I multiply <c>pmf</c> by the likelihoods and normalize.
    </p>

    <p>
      Here's how we can use this function to compute the updates in the previous section.
      I start with a fresh copy of the prior distribution:
    </p>

    <listing xml:id="list-dice-copy">
      <program language="python">
        <input>
pmf = prior.copy()
pmf
        </input>
      </program>
    </listing>
<table xml:id="tbl-dice-copy">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>
    <row>
      <cell>6</cell>
      <cell>0.333333</cell>
    </row>
    <row>
      <cell>8</cell>
      <cell>0.333333</cell>
    </row>
    <row>
      <cell>12</cell>
      <cell>0.333333</cell>
    </row>
  </tabular>
</table>

    <p>
      And use <c>update_dice</c> to do the updates.
    </p>

    <listing xml:id="list-dice-updates">
      <program language="python">
        <input>
update_dice(pmf, 1)
update_dice(pmf, 7)
pmf
        </input>
      </program>
    </listing>
<table xml:id="tbl-dice-updates">
  <tabular>
    <row>
      <cell></cell>
      <cell>probs</cell>
    </row>
    <row>
      <cell>6</cell>
      <cell>0.000000</cell>
    </row>
    <row>
      <cell>8</cell>
      <cell>0.692308</cell>
    </row>
    <row>
      <cell>12</cell>
      <cell>0.307692</cell>
    </row>
  </tabular>
</table>

    <p>
      The result is the same.
      We will see a version of this function in the next chapter.
    </p>
  </section>

  <section xml:id="sec-distributions-summary">
    <title>Summary</title>

    <p>
      This chapter introduces the <c>empiricaldist</c> module, which provides <c>Pmf</c>, which we use to represent a set of hypotheses and their probabilities.
    </p>

    <p>
      <c>empiricaldist</c> is based on Pandas; the <c>Pmf</c> class inherits from the Pandas <c>Series</c> class and provides additional features specific to probability mass functions.
      We'll use <c>Pmf</c> and other classes from <c>empiricaldist</c> throughout the book because they simplify the code and make it more readable.
      But we could do the same things directly with Pandas.
    </p>

    <p>
      We use a <c>Pmf</c> to solve the cookie problem and the dice problem, which we saw in the previous chapter.
      With a <c>Pmf</c> it is easy to perform sequential updates with multiple pieces of data.
    </p>

    <p>
      We also solved a more general version of the cookie problem, with 101 bowls rather than two.
      Then we computed the MAP, which is the quantity with the highest posterior probability.
    </p>

    <p>
      In the next chapter, I'll introduce the Euro problem, and we will use the binomial distribution.
      And, at last, we will make the leap from using Bayes's Theorem to doing Bayesian statistics.
    </p>

    <p>
      But first you might want to work on the exercises.
    </p>
  </section>

  <exercises xml:id="exercises-distributions">
    <title>Exercises</title>

    <exercise xml:id="ex-distributions-dice">
      <title>The Dice Problem</title>
      <statement>
        <p>
          Suppose I have a box with a 6-sided die, an 8-sided die, and a 12-sided die.
          I choose one of the dice at random, roll it four times, and get 1, 3, 5, and 7.
          What is the probability that I chose the 8-sided die?
        </p>
        <p>
          You can use the <c>update_dice</c> function or do the update yourself.
        </p>
      </statement>
    </exercise>

    <exercise xml:id="ex-distributions-dice-unequal">
      <title>Unequal Priors</title>
      <statement>
        <p>
          In the previous version of the dice problem, the prior probabilities are the same because the box contains one of each die.
          But suppose the box contains 1 die that is 4-sided, 2 dice that are 6-sided, 3 dice that are 8-sided, 4 dice that are 12-sided, and 5 dice that are 20-sided.
          I choose a die, roll it, and get a 7.
          What is the probability that I chose an 8-sided die?
        </p>
        <p>
          Hint: To make the prior distribution, call <c>Pmf</c> with two parameters.
        </p>
      </statement>
    </exercise>

    <exercise xml:id="ex-distributions-socks">
      <title>Sock Drawers</title>
      <statement>
        <p>
          Suppose I have two sock drawers.
          One contains equal numbers of black and white socks.
          The other contains equal numbers of red, green, and blue socks.
          Suppose I choose a drawer at random, choose two socks at random, and I tell you that I got a matching pair.
          What is the probability that the socks are white?
        </p>
        <p>
          For simplicity, let's assume that there are so many socks in both drawers that removing one sock makes a negligible change to the proportions.
        </p>
      </statement>
    </exercise>

    <exercise xml:id="ex-distributions-elvis">
      <title>Elvis's Twin</title>
      <statement>
        <p>
          Here's a problem from <em>Bayesian Data Analysis</em>:
        </p>
        <blockquote>
          <p>
            Elvis Presley had a twin brother (who died at birth).
            What is the probability that Elvis was an identical twin?
          </p>
        </blockquote>
        <p>
          Hint: In 1935, about 2/3 of twins were fraternal and 1/3 were identical.
        </p>
      </statement>
    </exercise>
  </exercises>

</chapter>
