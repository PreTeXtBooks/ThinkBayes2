<?xml version='1.0' encoding='utf-8'?>

<chapter xml:id="ch-conjugate-priors" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Conjugate Priors</title>

  <introduction>
    <p>In the previous chapters we have used grid approximations to solve a variety of problems.
One of my goals has been to show that this approach is sufficient to solve many real-world problems.
And I think it's a good place to start because it shows clearly how the methods work.</p>
    <p>However, as we saw in the previous chapter, grid methods will only get you so far.
As we increase the number of parameters, the number of points in the grid grows (literally) exponentially.
With more than 3-4 parameters, grid methods become impractical.</p>
    <p>So, in the remaining three chapters, I will present three alternatives:</p>
    <ol>
      <li><p>In this chapter we'll use <em>conjugate priors</em> to speed up some of the computations we've already done.</p></li>
      <li><p>In the next chapter, I'll present Markov chain Monte Carlo (MCMC) methods, which can solve problems with tens of parameters, or even hundreds, in a reasonable amount of time.</p></li>
      <li><p>And in the last chapter we'll use Approximate Bayesian Computation (ABC) for problems that are hard to model with simple distributions.</p></li>
    </ol>
    <p>We'll start with the World Cup problem.</p>
  </introduction>

  <section xml:id="sec-ch18-world-cup">
    <title>The World Cup Problem Revisited</title>

    <p>In <!-- ref: PoissonProcesses -->, we solved the World Cup problem using a Poisson process to model goals in a soccer game as random events that are equally likely to occur at any point during a game.</p>
    <p>We used a gamma distribution to represent the prior distribution of <m>\lambda</m>, the goal-scoring rate.  And we used a Poisson distribution to compute the probability of <m>k</m>, the number of goals scored.</p>
    <p>Here's a gamma object that represents the prior distribution.</p>
    <program language="python">
      <input>
from scipy.stats import gamma

alpha = 1.4
dist = gamma(alpha)
      </input>
    </program>
    <p>And here's a grid approximation.</p>
    <program language="python">
      <input>
import numpy as np
from utils import pmf_from_dist

lams = np.linspace(0, 10, 101)
prior = pmf_from_dist(dist, lams)
      </input>
    </program>
    <p>Here's the likelihood of scoring 4 goals for each possible value of <c>lam</c>.</p>
    <program language="python">
      <input>
from scipy.stats import poisson

k = 4
likelihood = poisson(lams).pmf(k)
      </input>
    </program>
    <p>And here's the update.</p>
    <program language="python">
      <input>
posterior = prior * likelihood
posterior.normalize()
      </input>
    </program>
<pre>
0.05015532557804499
</pre>
    <p>So far, this should be familiar.
Now we'll solve the same problem using the conjugate prior.</p>
  </section>

  <section xml:id="sec-ch18-conjugate-prior">
    <title>The Conjugate Prior</title>

    <p>In <!-- ref: TheGammaDistribution -->, I presented three reasons to use a gamma distribution for the prior and said there was a fourth reason I would reveal later.
Well, now is the time.</p>
    <p>The other reason I chose the gamma distribution is that it is the "conjugate prior" of the Poisson distribution, so-called because the two distributions are connected or coupled, which is what "conjugate" means.</p>
    <p>In the next section I'll explain <em>how</em> they are connected, but first I'll show you the consequence of this connection, which is that there is a remarkably simple way to compute the posterior distribution.</p>
    <p>However, in order to demonstrate it, we have to switch from the one-parameter version of the gamma distribution to the two-parameter version.  Since the first parameter is called <c>alpha</c>, you might guess that the second parameter is called <c>beta</c>.</p>
    <p>The following function takes <c>alpha</c> and <c>beta</c> and makes an object that represents a gamma distribution with those parameters.</p>
    <program language="python">
      <input>
def make_gamma_dist(alpha, beta):
    """Makes a gamma object."""
    dist = gamma(alpha, scale=1/beta)
    dist.alpha = alpha
    dist.beta = beta
    return dist
      </input>
    </program>
    <p>Here's the prior distribution with <c>alpha=1.4</c> again and <c>beta=1</c>.</p>
    <program language="python">
      <input>
alpha = 1.4
beta = 1

prior_gamma = make_gamma_dist(alpha, beta)
prior_gamma.mean()
      </input>
    </program>
<pre>
1.4
</pre>
    <p>Now I claim without proof that we can do a Bayesian update with <c>k</c> goals just by making a gamma distribution with parameters <c>alpha+k</c> and <c>beta+1</c>.</p>
    <program language="python">
      <input>
def update_gamma(prior, data):
    """Update a gamma prior."""
    k, t = data
    alpha = prior.alpha + k
    beta = prior.beta + t
    return make_gamma_dist(alpha, beta)
      </input>
    </program>
    <p>Here's how we update it with <c>k=4</c> goals in <c>t=1</c> game.</p>
    <program language="python">
      <input>
data = 4, 1
posterior_gamma = update_gamma(prior_gamma, data)
      </input>
    </program>
    <p>After all the work we did with the grid, it might seem absurd that we can do a Bayesian update by adding two pairs of numbers.
So let's confirm that it works.</p>
    <p>I'll make a <c>Pmf</c> with a discrete approximation of the posterior distribution.</p>
    <program language="python">
      <input>
posterior_conjugate = pmf_from_dist(posterior_gamma, lams)
      </input>
    </program>
    <p>The following figure shows the result along with the posterior we computed using the grid algorithm.</p>
    <program language="python">
      <input>
from utils import decorate

def decorate_rate(title=''):
    decorate(xlabel='Goal scoring rate (lam)',
             ylabel='PMF',
             title=title)
      </input>
    </program>
    <program language="python">
      <input>
posterior.plot(label='grid posterior', color='C1')
posterior_conjugate.plot(label='conjugate posterior', 
                         color='C4', ls=':')

decorate_rate('Posterior distribution')
      </input>
    </program>
    <image source="images/ch18_conjugate_priors_ea9966aa.png" width="80%"/>
    <p>They are the same other than small differences due to floating-point approximations.</p>
    <program language="python">
      <input>
np.allclose(posterior, posterior_conjugate)
      </input>
    </program>
<pre>
True
</pre>
  </section>

  <section xml:id="sec-ch18-what-actual">
    <title>What the Actual?</title>

    <p>To understand how that works, we'll write the PDF of the gamma prior and the PMF of the Poisson likelihood, then multiply them together, because that's what the Bayesian update does.
We'll see that the result is a gamma distribution, and we'll derive its parameters.</p>
    <p>Here's the PDF of the gamma prior, which is the probability density for each value of <m>\lambda</m>, given parameters <m>\alpha</m> and <m>\beta</m>:</p>
    <me>\lambda^{\alpha-1} e^{-\lambda \beta}</me>
    <p>I have omitted the normalizing factor; since we are planning to normalize the posterior distribution anyway, we don't really need it.</p>
    <p>Now suppose a team scores <m>k</m> goals in <m>t</m> games.
The probability of this data is given by the PMF of the Poisson distribution, which is a function of <m>k</m> with <m>\lambda</m> and <m>t</m> as parameters.</p>
    <me>\lambda^k e^{-\lambda t}</me>
    <p>Again, I have omitted the normalizing factor, which makes it clearer that the gamma and Poisson distributions have the same functional form.
When we multiply them together, we can pair up the factors and add up the exponents.
The result is the unnormalized posterior distribution,</p>
    <me>\lambda^{\alpha-1+k} e^{-\lambda(\beta + t)}</me>
    <p>which we can recognize as an unnormalized gamma distribution with parameters <m>\alpha + k</m> and <m>\beta + t</m>.</p>
    <p>This derivation provides insight into what the parameters of the posterior distribution mean: <m>\alpha</m> reflects the number of events that have occurred; <m>\beta</m> reflects the elapsed time.</p>
  </section>

  <section xml:id="sec-ch18-binomial">
    <title>Binomial Likelihood</title>

    <p>As a second example, let's look again at the Euro problem.
When we solved it with a grid algorithm, we started with a uniform prior:</p>
    <program language="python">
      <input>
from utils import make_uniform

xs = np.linspace(0, 1, 101)
uniform = make_uniform(xs, 'uniform')
      </input>
    </program>
    <p>We used the binomial distribution to compute the likelihood of the data, which was 140 heads out of 250 attempts.</p>
    <program language="python">
      <input>
from scipy.stats import binom

k, n = 140, 250
xs = uniform.qs
likelihood = binom.pmf(k, n, xs)
      </input>
    </program>
    <p>Then we computed the posterior distribution in the usual way.</p>
    <program language="python">
      <input>
posterior = uniform * likelihood
posterior.normalize()
      </input>
    </program>
<pre>
0.003944617569326651
</pre>
    <p>We can solve this problem more efficiently using the conjugate prior of the binomial distribution, which is the beta distribution.</p>
    <p>The beta distribution is bounded between 0 and 1, so it works well for representing the distribution of a probability like <c>x</c>.
It has two parameters, called <c>alpha</c> and <c>beta</c>, that determine the shape of the distribution.</p>
    <p>SciPy provides an object called <c>beta</c> that represents a beta distribution.
The following function takes <c>alpha</c> and <c>beta</c> and returns a new <c>beta</c> object.</p>
    <program language="python">
      <input>
import scipy.stats

def make_beta(alpha, beta):
    """Makes a beta object."""
    dist = scipy.stats.beta(alpha, beta)
    dist.alpha = alpha
    dist.beta = beta
    return dist
      </input>
    </program>
    <p>It turns out that the uniform distribution, which we used as a prior, is the beta distribution with parameters <c>alpha=1</c> and <c>beta=1</c>.
So we can make a <c>beta</c> object that represents a uniform distribution, like this:</p>
    <program language="python">
      <input>
alpha = 1
beta = 1

prior_beta = make_beta(alpha, beta)
      </input>
    </program>
    <p>Now let's figure out how to do the update.  As in the previous example, we'll write the PDF of the prior distribution and the PMF of the likelihood function, and multiply them together.  We'll see that the product has the same form as the prior, and we'll derive its parameters.</p>
    <p>Here is the PDF of the beta distribution, which is a function of <m>x</m> with <m>\alpha</m> and <m>\beta</m> as parameters.</p>
    <me>x^{\alpha-1} (1-x)^{\beta-1}</me>
    <p>Again, I have omitted the normalizing factor, which we don't need because we are going to normalize the distribution after the update.</p>
    <p>And here's the PMF of the binomial distribution, which is a function of <m>k</m> with <m>n</m> and <m>x</m> as parameters.</p>
    <me>x^{k} (1-x)^{n-k}</me>
    <p>Again, I have omitted the normalizing factor.
Now when we multiply the beta prior and the binomial likelihood, the result is</p>
    <me>x^{\alpha-1+k} (1-x)^{\beta-1+n-k}</me>
    <p>which we recognize as an unnormalized beta distribution with parameters <m>\alpha+k</m> and <m>\beta+n-k</m>.</p>
    <p>So if we observe <c>k</c> successes in <c>n</c> trials, we can do the update by making a beta distribution with parameters <c>alpha+k</c> and <c>beta+n-k</c>.
That's what this function does:</p>
    <program language="python">
      <input>
def update_beta(prior, data):
    """Update a beta distribution."""
    k, n = data
    alpha = prior.alpha + k
    beta = prior.beta + n - k
    return make_beta(alpha, beta)
      </input>
    </program>
    <p>Again, the conjugate prior gives us insight into the meaning of the parameters; <m>\alpha</m> is related to the number of observed successes; <m>\beta</m> is related to the number of failures.</p>
    <p>Here's how we do the update with the observed data.</p>
    <program language="python">
      <input>
data = 140, 250
posterior_beta = update_beta(prior_beta, data)
      </input>
    </program>
    <p>To confirm that it works, I'll evaluate the posterior distribution for the possible values of <c>xs</c> and put the results in a <c>Pmf</c>.</p>
    <program language="python">
      <input>
posterior_conjugate = pmf_from_dist(posterior_beta, xs)
      </input>
    </program>
    <p>And we can compare the posterior distribution we just computed with the results from the grid algorithm.</p>
    <program language="python">
      <input>
def decorate_euro(title):
    decorate(xlabel='Proportion of heads (x)',
             ylabel='Probability',
             title=title)
      </input>
    </program>
    <program language="python">
      <input>
posterior.plot(label='grid posterior', color='C1')
posterior_conjugate.plot(label='conjugate posterior',
                        color='C4', ls=':')

decorate_euro(title='Posterior distribution of x')
      </input>
    </program>
    <image source="images/ch18_conjugate_priors_ca57b83c.png" width="80%"/>
    <p>They are the same other than small differences due to floating-point approximations.</p>
    <p>The examples so far are problems we have already solved, so let's try something new.</p>
    <program language="python">
      <input>
np.allclose(posterior, posterior_conjugate)
      </input>
    </program>
<pre>
True
</pre>
  </section>

  <section xml:id="sec-ch18-lions-tigers">
    <title>Lions and Tigers and Bears</title>

    <p>Suppose we visit a wild animal preserve where we know that the only animals are lions and tigers and bears, but we don't know how many of each there are.
During the tour, we see 3 lions, 2 tigers, and one bear. Assuming that every animal had an equal chance to appear in our sample, what is the probability that the next animal we see is a bear?</p>
    <p>To answer this question, we'll use the data to estimate the prevalence of each species, that is, what fraction of the animals belong to each species.
If we know the prevalences, we can use the multinomial distribution to compute the probability of the data.
For example, suppose we know that the fraction of lions, tigers, and bears is 0.4, 0.3, and 0.3, respectively.</p>
    <p>In that case the probability of the data is:</p>
    <program language="python">
      <input>
from scipy.stats import multinomial

data = 3, 2, 1
n = np.sum(data)
ps = 0.4, 0.3, 0.3

multinomial.pmf(data, n, ps)
      </input>
    </program>
<pre>
0.10368
</pre>
    <p>Now, we could choose a prior for the prevalences and do a Bayesian update using the multinomial distribution to compute the probability of the data.</p>
    <p>But there's an easier way, because the multinomial distribution has a conjugate prior: the Dirichlet distribution.</p>
  </section>

  <section xml:id="sec-ch18-dirichlet">
    <title>The Dirichlet Distribution</title>

    <p>The Dirichlet distribution is a multivariate distribution, like the multivariate normal distribution we used in <!-- ref: MultivariateNormalDistribution --> to describe the distribution of penguin measurements.</p>
    <p>In that example, the quantities in the distribution are pairs of flipper length and culmen length, and the parameters of the distribution are a vector of means and a matrix of covariances.</p>
    <p>In a Dirichlet distribution, the quantities are vectors of probabilities, <m>\mathbf{x}</m>, and the parameter is a vector, <m>\mathbf{\alpha}</m>.</p>
    <p>An example will make that clearer.  SciPy provides a <c>dirichlet</c> object that represents a Dirichlet distribution.
Here's an instance with <m>\mathbf{\alpha} = 1, 2, 3</m>.</p>
    <program language="python">
      <input>
from scipy.stats import dirichlet

alpha = 1, 2, 3
dist = dirichlet(alpha)
      </input>
    </program>
    <p>Since we provided three parameters, the result is a distribution of three variables.
If we draw a random value from this distribution, like this:</p>
    <program language="python">
      <input>
dist.rvs()
      </input>
    </program>
<pre>
array([[0.00389931, 0.49015107, 0.50594962]])
</pre>
    <program language="python">
      <input>
dist.rvs().sum()
      </input>
    </program>
<pre>
1.0000000000000002
</pre>
    <p>The result is an array of three values. 
They are bounded between 0 and 1, and they always add up to 1, so they can be interpreted as the probabilities of a set of outcomes that are mutually exclusive and collectively exhaustive.</p>
    <p>Let's see what the distributions of these values look like.  I'll draw 1000 random vectors from this distribution, like this:</p>
    <program language="python">
      <input>
sample = dist.rvs(1000)
      </input>
    </program>
    <program language="python">
      <input>
sample.shape
      </input>
    </program>
<pre>
(1000, 3)
</pre>
    <p>The result is an array with 1000 rows and three columns.  I'll compute the <c>Cdf</c> of the values in each column.</p>
    <program language="python">
      <input>
from empiricaldist import Cdf

cdfs = [Cdf.from_seq(col) 
        for col in sample.transpose()]
      </input>
    </program>
    <p>The result is a list of <c>Cdf</c> objects that represent the marginal distributions of the three variables.  Here's what they look like.</p>
    <program language="python">
      <input>
for i, cdf in enumerate(cdfs):
    label = f'Column {i}'
    cdf.plot(label=label)
    
decorate()
      </input>
    </program>
    <image source="images/ch18_conjugate_priors_b7175728.png" width="80%"/>
    <p>Column 0, which corresponds to the lowest parameter, contains the lowest probabilities.
Column 2, which corresponds to the highest parameter, contains the highest probabilities.</p>
    <p>As it turns out, these marginal distributions are beta distributions.
The following function takes a sequence of parameters, <c>alpha</c>, and computes the marginal distribution of variable <c>i</c>:</p>
    <program language="python">
      <input>
def marginal_beta(alpha, i):
    """Compute the ith marginal of a Dirichlet distribution."""
    total = np.sum(alpha)
    return make_beta(alpha[i], total-alpha[i])
      </input>
    </program>
    <p>We can use it to compute the marginal distribution for the three variables.</p>
    <program language="python">
      <input>
marginals = [marginal_beta(alpha, i)
             for i in range(len(alpha))]
      </input>
    </program>
    <p>The following plot shows the CDF of these distributions as gray lines and compares them to the CDFs of the samples.</p>
    <program language="python">
      <input>
xs = np.linspace(0, 1, 101)

for i in range(len(alpha)):
    label = f'Column {i}'
    
    pmf = pmf_from_dist(marginals[i], xs)
    pmf.make_cdf().plot(color='C5')
    
    cdf = cdfs[i]
    cdf.plot(label=label, ls=':')

decorate()
      </input>
    </program>
    <image source="images/ch18_conjugate_priors_a63c6354.png" width="80%"/>
    <p>This confirms that the marginals of the Dirichlet distribution are beta distributions.
And that's useful because the Dirichlet distribution is the conjugate prior for the multinomial likelihood function.</p>
    <p>If the prior distribution is Dirichlet with parameter vector <c>alpha</c> and the data is a vector of observations, <c>data</c>, the posterior distribution is Dirichlet with parameter vector <c>alpha + data</c>.</p>
    <p>As an exercise at the end of this chapter, you can use this method to solve the Lions and Tigers and Bears problem.</p>
  </section>

  <section xml:id="sec-ch18-summary">
    <title>Summary</title>

    <p>After reading this chapter, if you feel like you've been tricked, I understand.  It turns out that many of the problems in this book can be solved with just a few arithmetic operations.  So why did we go to all the trouble of using grid algorithms?</p>
    <p>Sadly, there are only a few problems we can solve with conjugate priors; in fact, this chapter includes most of the ones that are useful in practice.</p>
    <p>For the vast majority of problems, there is no conjugate prior and no shortcut to compute the posterior distribution.
That's why we need grid algorithms and the methods in the next two chapters, Approximate Bayesian Computation (ABC) and Markov chain Monte Carlo methods (MCMC).</p>
  </section>

  <section xml:id="sec-ch18-exercises">
    <title>Exercises</title>

    <exercise xml:id="ex-ch18-1">
      <title>Exercise 1</title>
      <statement>
        <p>In the second version of the World Cup problem, the data we use for the update is not the number of goals in a game, but the time until the first goal.
So the probability of the data is given by the exponential distribution rather than the Poisson distribution.</p>
        <p>But it turns out that the gamma distribution is <em>also</em> the conjugate prior of the exponential distribution, so there is a simple way to compute this update, too.
The PDF of the exponential distribution is a function of <m>t</m> with <m>\lambda</m> as a parameter.</p>
        <me>\lambda e^{-\lambda t}</me>
        <p>Multiply the PDF of the gamma prior by this likelihood, confirm that the result is an unnormalized gamma distribution, and see if you can derive its parameters.</p>
        <p>Write a few lines of code to update <c>prior_gamma</c> with the data from this version of the problem, which was a first goal after 11 minutes and a second goal after an additional 12 minutes.</p>
        <p>Remember to express these quantities in units of games, which are approximately 90 minutes.</p>
      </statement>
    <solution>
      <program language="python">
        <input>

"""
The unnormalized posterior is

\lambda^{\alpha-1+1} e^{-(\beta + t) \lambda}

which is an unnormalized gamma distribution with parameters
`alpha+1` and `beta+t`, which means that we observed 1 goal
in elapsed time `t`.

So we can use the same update function and call it like this:
"""

data = 1, 11/90
posterior1 = update_gamma(prior_gamma, data)
        </input>
      </program>
      <program language="python">
        <input>

data = 1, 12/90
posterior2 = update_gamma(posterior1, data)
        </input>
      </program>
      <program language="python">
        <input>

prior_gamma.mean(), posterior1.mean(), posterior2.mean()
        </input>
      </program>
      <program language="python">
        <input>

pmf_from_dist(prior_gamma, lams).plot(color='C5', label='prior')
pmf_from_dist(posterior1, lams).plot(label='after 1 goal')
pmf_from_dist(posterior2, lams).plot(label='after 2 goals')

decorate_rate(title='World Cup Problem, Germany v Brazil')
        </input>
      </program>
    </solution>
    </exercise>

    <exercise xml:id="ex-ch18-2">
      <title>Exercise 2</title>
      <statement>
        <p>For problems like the Euro problem where the likelihood function is binomial, we can do a Bayesian update with just a few arithmetic operations, but only if the prior is a beta distribution.</p>
        <p>If we want a uniform prior, we can use a beta distribution with <c>alpha=1</c> and <c>beta=1</c>.
But what can we do if the prior distribution we want is not a beta distribution?
For example, in <!-- ref: TrianglePrior --> we also solved the Euro problem with a triangle prior, which is not a beta distribution.</p>
        <p>In these cases, we can often find a beta distribution that is a good-enough approximation for the prior we want.
See if you can find a beta distribution that fits the triangle prior, then update it using <c>update_beta</c>.</p>
        <p>Use <c>pmf_from_dist</c> to make a <c>Pmf</c> that approximates the posterior distribution and compare it to the posterior we just computed using a grid algorithm.  How big is the largest difference between them?</p>
        <p>Here's the triangle prior again.</p>
        <program language="python">
          <input>
from empiricaldist import Pmf

ramp_up = np.arange(50)
ramp_down = np.arange(50, -1, -1)

a = np.append(ramp_up, ramp_down)
xs = uniform.qs

triangle = Pmf(a, xs, name='triangle')
triangle.normalize()
          </input>
    </program>
<pre>
2500
</pre>
        <p>And here's the update.</p>
        <program language="python">
          <input>
k, n = 140, 250
likelihood = binom.pmf(k, n, xs)

posterior = triangle * likelihood
posterior.normalize()
          </input>
    </program>
<pre>
0.007008842590059087
</pre>
        <p>To get you started, here's the beta distribution that we used as a uniform prior.</p>
        <program language="python">
          <input>
alpha = 1
beta = 1

prior_beta = make_beta(alpha, beta)
prior_beta.mean()
          </input>
    </program>
<pre>
0.5
</pre>
        <p>And here's what it looks like compared to the triangle prior.</p>
        <program language="python">
          <input>
prior_pmf = pmf_from_dist(prior_beta, xs)

triangle.plot(label='triangle')
prior_pmf.plot(label='beta')

decorate_euro('Prior distributions')
          </input>
        </program>
    <image source="images/ch18_conjugate_priors_47e2d565.png" width="80%"/>
        <p>Now you take it from there.</p>
      </statement>
    <solution>
      <program language="python">
        <input>
from empiricaldist import Pmf

ramp_up = np.arange(50)
ramp_down = np.arange(50, -1, -1)

a = np.append(ramp_up, ramp_down)
xs = uniform.qs

triangle = Pmf(a, xs, name='triangle')
triangle.normalize()
        </input>
    </program>
<pre>
2500
</pre>
      <program language="python">
        <input>
k, n = 140, 250
likelihood = binom.pmf(k, n, xs)

posterior = triangle * likelihood
posterior.normalize()
        </input>
    </program>
<pre>
0.007008842590059087
</pre>
      <program language="python">
        <input>
alpha = 1
beta = 1

prior_beta = make_beta(alpha, beta)
prior_beta.mean()
        </input>
    </program>
<pre>
0.5
</pre>
      <program language="python">
        <input>
prior_pmf = pmf_from_dist(prior_beta, xs)

triangle.plot(label='triangle')
prior_pmf.plot(label='beta')

decorate_euro('Prior distributions')
        </input>
      </program>
    <image source="images/ch18_conjugate_priors_47e2d565.png" width="80%"/>
      <program language="python">
        <input>

data = 140, 250
posterior_beta = update_beta(prior_beta, data)
posterior_beta.mean()
        </input>
      </program>
      <program language="python">
        <input>

posterior_conjugate = pmf_from_dist(posterior_beta, xs)
        </input>
      </program>
      <program language="python">
        <input>

posterior.plot(label='grid posterior', ls=':')
posterior_conjugate.plot(label='conjugate posterior')

decorate(xlabel='Proportion of heads (x)',
         ylabel='Probability',
         title='Posterior distribution of x')
        </input>
      </program>
      <program language="python">
        <input>

np.allclose(posterior, posterior_conjugate)
        </input>
    </program>
<pre>
True
</pre>
    </solution>
    </exercise>

    <exercise xml:id="ex-ch18-3">
      <title>Exercise 3</title>
      <statement>
        <p><url href="https://en.wikipedia.org/wiki/3Blue1Brown">3Blue1Brown</url> is a YouTube channel about math; if you are not already aware of it, I recommend it highly.
In <url href="https://www.youtube.com/watch?v=8idr1WZ1A7Q">this video</url> the narrator presents this problem:</p>
        <blockquote>
          <p>You are buying a product online and you see three sellers offering the same product at the same price.  One of them has a 100% positive rating, but with only 10 reviews.  Another has a 96% positive rating with 50 total reviews.  And yet another has a 93% positive rating, but with 200 total reviews.</p>
          <p>Which one should you buy from?</p>
        </blockquote>
        <p>Let's think about how to model this scenario.  Suppose each seller has some unknown probability, <c>x</c>, of providing satisfactory service and getting a positive rating, and we want to choose the seller with the highest value of <c>x</c>.</p>
        <p>This is not the only model for this scenario, and it is not necessarily the best.  An alternative would be something like item response theory, where sellers have varying ability to provide satisfactory service and customers have varying difficulty of being satisfied.</p>
        <p>But the first model has the virtue of simplicity, so let's see where it gets us.</p>
        <p>1. As a prior, I suggest a beta distribution with <c>alpha=8</c> and <c>beta=2</c>.  What does this prior look like and what does it imply about sellers?</p>
        <p>2. Use the data to update the prior for the three sellers and plot the posterior distributions.  Which seller has the highest posterior mean?</p>
        <p>3. How confident should we be about our choice?  That is, what is the probability that the seller with the highest posterior mean actually has the highest value of <c>x</c>?</p>
        <p>4. Consider a beta prior with <c>alpha=0.7</c> and <c>beta=0.5</c>.  What does this prior look like and what does it imply about sellers?</p>
        <p>5. Run the analysis again with this prior and see what effect it has on the results.</p>
        <p>Note: When you evaluate the beta distribution, you should restrict the range of <c>xs</c> so it does not include 0 and 1.  When the parameters of the beta distribution are less than 1, the probability density goes to infinity at 0 and 1.  From a mathematical point of view, that's not a problem; it is still a proper probability distribution.  But from a computational point of view, it means we have to avoid evaluating the PDF at 0 and 1.</p>
      </statement>
    <solution>
      <program language="python">
        <input>

prior = make_beta(8, 2)

xs = np.linspace(0.005, 0.995, 199)
prior_pmf = pmf_from_dist(prior, xs)
prior_pmf.plot(color='C5', label='prior')

decorate(xlabel='Probability of positive rating',
         ylabel='PDF')
        </input>
      </program>
      <program language="python">
        <input>

data1 = 10, 10
data2 = 48, 50
data3 = 186, 200
        </input>
      </program>
      <program language="python">
        <input>

seller1 = update_beta(prior, data1)
seller2 = update_beta(prior, data2)
seller3 = update_beta(prior, data3)
        </input>
      </program>
      <program language="python">
        <input>

seller1_pmf = pmf_from_dist(seller1, xs)
seller2_pmf = pmf_from_dist(seller2, xs)
seller3_pmf = pmf_from_dist(seller3, xs)
        </input>
      </program>
      <program language="python">
        <input>

seller1_pmf.plot(label='seller 1')
seller2_pmf.plot(label='seller 2')
seller3_pmf.plot(label='seller 3')

decorate(xlabel='Probability of positive rating',
         ylabel='PDF',
         xlim=(0.65, 1.0))
        </input>
      </program>
      <program language="python">
        <input>

seller1.mean(), seller2.mean(), seller3.mean()
        </input>
      </program>
      <program language="python">
        <input>

iters = 10000
a = np.empty((3, iters))

a[0] = seller1.rvs(iters)
a[1] = seller2.rvs(iters)
a[2] = seller3.rvs(iters)
        </input>
      </program>
      <program language="python">
        <input>

from empiricaldist import Pmf

best = np.argmax(a, axis=0)
Pmf.from_seq(best)
        </input>
      </program>
    </solution>
    </exercise>

    <exercise xml:id="ex-ch18-4">
      <title>Exercise 4</title>
      <statement>
        <p>Use a Dirichlet prior with parameter vector <c>alpha = [1, 1, 1]</c> to solve the Lions and Tigers and Bears problem:</p>
        <blockquote>
          <p>Suppose we visit a wild animal preserve where we know that the only animals are lions and tigers and bears, but we don't know how many of each there are.</p>
          <p>During the tour, we see three lions, two tigers, and one bear. Assuming that every animal had an equal chance to appear in our sample, estimate the prevalence of each species.</p>
          <p>What is the probability that the next animal we see is a bear?</p>
        </blockquote>
        <p><em>Think Bayes</em>, Second Edition</p>
        <p>Copyright 2020 Allen B. Downey</p>
        <p>License: <url href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</url></p>
      </statement>
    <solution>
      <program language="python">
        <input>

prior_alpha = np.array([1, 1, 1])
data = 3, 2, 1
        </input>
      </program>
      <program language="python">
        <input>

posterior_alpha = prior_alpha + data
        </input>
      </program>
      <program language="python">
        <input>

marginal_bear = marginal_beta(posterior_alpha, 2)
marginal_bear.mean()
        </input>
      </program>
      <program language="python">
        <input>

dist = dirichlet(posterior_alpha)
        </input>
      </program>
      <program language="python">
        <input>

import pandas as pd

index = ['lion', 'tiger', 'bear']
pd.DataFrame(dist.mean(), index, columns=['prob'])
        </input>
      </program>
    </solution>
    </exercise>

  </section>

</chapter>
