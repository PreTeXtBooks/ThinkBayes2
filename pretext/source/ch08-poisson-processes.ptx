<?xml version='1.0' encoding='utf-8'?>

<chapter xml:id="ch-poisson-processes" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Poisson Processes</title>

  <introduction>
    <p>
      This chapter introduces the <url href="https://en.wikipedia.org/wiki/Poisson_point_process">Poisson process</url>, which is a model used to describe events that occur at random intervals.
      As an example of a Poisson process, we'll model goal-scoring in soccer, which is American English for the game everyone else calls <q>football</q>.
      We'll use goals scored in a game to estimate the parameter of a Poisson process; then we'll use the posterior distribution to make predictions.
    </p>

    <p>
      And we'll solve The World Cup Problem.
    </p>
  </introduction>

  <section xml:id="sec-poisson-world-cup">
    <title>The World Cup Problem</title>

    <p>
      In the 2018 FIFA World Cup final, France defeated Croatia 4 goals to 2.  Based on this outcome:
    </p>

    <ol>
      <li><p>How confident should we be that France is the better team?</p></li>
      <li><p>If the same teams played again, what is the chance France would win again?</p></li>
    </ol>

    <p>
      To answer these questions, we have to make some modeling decisions.
    </p>

    <p>
      First, I'll assume that for any team against another team there is some unknown goal-scoring rate, measured in goals per game, which I'll denote with the Python variable <c>lam</c> or the Greek letter <m>\lambda</m>, pronounced <q>lambda</q>.
    </p>

    <p>
      Second, I'll assume that a goal is equally likely during any minute of a game.  So, in a 90 minute game, the probability of scoring during any minute is <m>\lambda / 90</m>.
    </p>

    <p>
      Third, I'll assume that a team never scores twice during the same minute.
    </p>

    <p>
      Of course, none of these assumptions is completely true in the real world, but I think they are reasonable simplifications.
      As George Box said, <q>All models are wrong; some are useful.</q>
      (<url href="https://en.wikipedia.org/wiki/All_models_are_wrong">https://en.wikipedia.org/wiki/All_models_are_wrong</url>).
    </p>

    <p>
      In this case, the model is useful because if these assumptions are true, at least roughly, the number of goals scored in a game follows a Poisson distribution, at least roughly.
    </p>
  </section>

  <section xml:id="sec-poisson-distribution">
    <title>The Poisson Distribution</title>

    <p>
      If the number of goals scored in a game follows a <url href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</url> with a goal-scoring rate, <m>\lambda</m>, the probability of scoring <m>k</m> goals is
    </p>

    <me>
      \lambda^k \exp(-\lambda) ~/~ k!
    </me>

    <p>
      for any non-negative value of <m>k</m>.
    </p>

    <p>
      SciPy provides a <c>poisson</c> object that represents a Poisson distribution.
      We can create one with <m>\lambda=1.4</m> like this:
    </p>

    <program language="python">
      <input>
from scipy.stats import poisson

lam = 1.4
dist = poisson(lam)
type(dist)
      </input>
    </program>

    <p>
      The result is an object that represents a <q>frozen</q> random variable and provides <c>pmf</c>, which evaluates the probability mass function of the Poisson distribution.
    </p>

    <program language="python">
      <input>
k = 4
dist.pmf(k)
      </input>
    </program>

    <p>
      This result implies that if the average goal-scoring rate is 1.4 goals per game, the probability of scoring 4 goals in a game is about 4%.
    </p>

    <p>
      We'll use the following function to make a <c>Pmf</c> that represents a Poisson distribution.
    </p>

    <program language="python">
      <input>
from empiricaldist import Pmf

def make_poisson_pmf(lam, qs):
    """Make a Pmf of a Poisson distribution."""
    ps = poisson(lam).pmf(qs)
    pmf = Pmf(ps, qs)
    pmf.normalize()
    return pmf
      </input>
    </program>

    <p>
      <c>make_poisson_pmf</c> takes as parameters the goal-scoring rate, <c>lam</c>, and an array of quantities, <c>qs</c>, where it should evaluate the Poisson PMF.  It returns a <c>Pmf</c> object.
    </p>

    <p>
      For example, here's the distribution of goals scored for <c>lam=1.4</c>, computed for values of <c>k</c> from 0 to 9.
    </p>

    <program language="python">
      <input>
import numpy as np

lam = 1.4
goals = np.arange(10)
pmf_goals = make_poisson_pmf(lam, goals)
      </input>
    </program>

    <p>
      And here's what it looks like.
    </p>

    <p>
      <!-- Figure fig07-01: Poisson distribution with lambda=1.4. -->
      The most likely outcomes are 0, 1, and 2; higher values are possible but increasingly unlikely.
      Values above 7 are negligible.
      This distribution shows that if we know the goal scoring rate, we can predict the number of goals.
    </p>

    <p>
      Now let's turn it around: given a number of goals, what can we say about the goal-scoring rate?
    </p>

    <p>
      To answer that, we need to think about the prior distribution of <c>lam</c>, which represents the range of possible values and their probabilities before we see the score.
    </p>
  </section>

  <section xml:id="sec-gamma-distribution">
    <title>The Gamma Distribution</title>

    <p>
      If you have ever seen a soccer game, you have some information about <c>lam</c>.
      In most games, teams score a few goals each.
      In rare cases, a team might score more than 5 goals, but they almost never score more than 10.
    </p>

    <p>
      Using <url href="https://www.statista.com/statistics/269031/goals-scored-per-game-at-the-fifa-world-cup-since-1930/">data from previous World Cups</url>, I estimate that each team scores about 1.4 goals per game, on average.  So I'll set the mean of <c>lam</c> to be 1.4.
    </p>

    <p>
      For a good team against a bad one, we expect <c>lam</c> to be higher; for a bad team against a good one, we expect it to be lower.
    </p>

    <p>
      To model the distribution of goal-scoring rates, I'll use a <url href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distribution</url>, which I chose because:
    </p>

    <ol>
      <li><p>The goal scoring rate is continuous and non-negative, and the gamma distribution is appropriate for this kind of quantity.</p></li>
      <li><p>The gamma distribution has only one parameter, <c>alpha</c>, which is the mean.  So it's easy to construct a gamma distribution with the mean we want.</p></li>
      <li><p>As we'll see, the shape of the gamma distribution is a reasonable choice, given what we know about soccer.</p></li>
      <li><p>And there's one more reason, which I will reveal in <xref ref="ch-conjugate-priors"/>.</p></li>
    </ol>

    <p>
      SciPy provides <c>gamma</c>, which creates an object that represents a gamma distribution.
      And the <c>gamma</c> object provides <c>pdf</c>, which evaluates the <term>probability density function</term> (PDF) of the gamma distribution.
      Here's how we use it.
    </p>

    <program language="python">
      <input>
from scipy.stats import gamma

alpha = 1.4
qs = np.linspace(0, 10, 101)
ps = gamma(alpha).pdf(qs)
      </input>
    </program>

    <p>
      The parameter, <c>alpha</c>, is the mean of the distribution.
      The <c>qs</c> are possible values of <c>lam</c> between 0 and 10.
      The <c>ps</c> are <term>probability densities</term>, which we can think of as unnormalized probabilities.
    </p>

    <p>
      To normalize them, we can put them in a <c>Pmf</c> and call <c>normalize</c>:
    </p>

    <program language="python">
      <input>
prior = Pmf(ps, qs)
prior.normalize()
      </input>
    </program>

    <p>
      The result is a discrete approximation of a gamma distribution.
      Here's what it looks like.
    </p>

    <p>
      <!-- Figure fig07-02: A gamma prior distribution of goal-scoring rate. -->
      This distribution represents our prior knowledge about goal scoring: <c>lam</c> is usually less than 2, occasionally as high as 6, and seldom higher than that.
    </p>

    <p>
      And we can confirm that the mean is about 1.4.
    </p>

    <p>
      As usual, reasonable people could disagree about the details of the prior, but this is good enough to get started.
      Let's do an update.
    </p>
  </section>

  <section xml:id="sec-poisson-update">
    <title>The Update</title>

    <p>
      Suppose you are given the goal-scoring rate, <m>\lambda</m>, and asked to compute the probability of scoring a number of goals, <m>k</m>.  That is precisely the question we answered by computing the Poisson PMF.
    </p>

    <p>
      For example, if <m>\lambda</m> is 1.4, the probability of scoring 4 goals in a game is:
    </p>

    <program language="python">
      <input>
lam = 1.4
k = 4
poisson(lam).pmf(4)
      </input>
    </program>

    <p>
      Now suppose we have an array of possible values for <m>\lambda</m>; we can compute the likelihood of the data for each hypothetical value of <c>lam</c>, like this:
    </p>

    <program language="python">
      <input>
lams = prior.qs
k = 4
likelihood = poisson(lams).pmf(k)
      </input>
    </program>

    <p>
      And that's all we need to do the update.
      To get the posterior distribution, we multiply the prior by the likelihoods we just computed and normalize the result.
      The following function encapsulates these steps.
    </p>

    <program language="python">
      <input>
def update_poisson(pmf, data):
    """Update Pmf with a Poisson likelihood."""
    k = data
    lams = pmf.qs
    likelihood = poisson(lams).pmf(k)
    pmf *= likelihood
    pmf.normalize()
      </input>
    </program>

    <p>
      The first parameter is the prior; the second is the number of goals.
    </p>

    <p>
      In the example, France scored 4 goals, so I'll make a copy of the prior and update it with the data.
    </p>

    <program language="python">
      <input>
france = prior.copy()
update_poisson(france, 4)
      </input>
    </program>

    <p>
      Here's what the posterior distribution looks like, along with the prior.
      The data, <c>k=4</c>, makes us think higher values of <c>lam</c> are more likely and lower values are less likely.  So the posterior distribution is shifted to the right.
    </p>

    <p>
      Let's do the same for Croatia:
    </p>

    <program language="python">
      <input>
croatia = prior.copy()
update_poisson(croatia, 2)
      </input>
    </program>

    <p>
      <!-- Figure fig07-03: Posterior distributions for France and Croatia. -->
      Here are the posterior means for these distributions.
    </p>

    <program language="python">
      <input>
print(croatia.mean(), france.mean())
      </input>
    </program>

    <p>
      The mean of the prior distribution is about 1.4.
      After Croatia scores 2 goals, their posterior mean is 1.7, which is near the midpoint of the prior and the data.
      Likewise after France scores 4 goals, their posterior mean is 2.7.
    </p>

    <p>
      These results are typical of a Bayesian update: the location of the posterior distribution is a compromise between the prior and the data.
    </p>
  </section>

  <section xml:id="sec-poisson-superiority">
    <title>Probability of Superiority</title>

    <p>
      Now that we have a posterior distribution for each team, we can answer the first question: How confident should we be that France is the better team?
    </p>

    <p>
      In the model, <q>better</q> means having a higher goal-scoring rate against the opponent.
      We can use the posterior distributions to compute the probability that a random value drawn from France's distribution exceeds a value drawn from Croatia's.
    </p>

    <p>
      One way to do that is to enumerate all pairs of values from the two distributions, adding up the total probability that one value exceeds the other.
    </p>

    <program language="python">
      <input>
def prob_gt(pmf1, pmf2):
    """Compute the probability of superiority."""
    total = 0
    for q1, p1 in pmf1.items():
        for q2, p2 in pmf2.items():
            if q1 > q2:
                total += p1 * p2
    return total
      </input>
    </program>

    <p>
      This is similar to the method we use in <xref ref="sec-ch06-addends"/> to compute the distribution of a sum.
      Here's how we use it:
    </p>

    <program language="python">
      <input>
prob_gt(france, croatia)
      </input>
    </program>

    <p>
      <c>Pmf</c> provides a function that does the same thing.
    </p>

    <program language="python">
      <input>
Pmf.prob_gt(france, croatia)
      </input>
    </program>

    <p>
      The results are slightly different because <c>Pmf.prob_gt</c> uses array operators rather than <c>for</c> loops.
    </p>

    <p>
      Either way, the result is close to 75%.  So, on the basis of one game, we have moderate confidence that France is actually the better team.
    </p>

    <p>
      Of course, we should remember that this result is based on the assumption that the goal-scoring rate is constant.
      In reality, if a team is down by one goal, they might play more aggressively toward the end of the game, making them more likely to score, but also more likely to give up an additional goal.
    </p>

    <p>
      As always, the results are only as good as the model.
    </p>
  </section>

  <section xml:id="sec-poisson-distribution-goals">
    <title>Predicting the Rematch</title>

    <p>
      Now we can take on the second question: If the same teams played again, what is the chance Croatia would win?
      To answer this question, we'll generate the <q>posterior predictive distribution</q>, which is the number of goals we expect a team to score.
    </p>

    <p>
      If we knew the goal scoring rate, <c>lam</c>, the distribution of goals would be a Poisson distribution with parameter <c>lam</c>.
      Since we don't know <c>lam</c>, the distribution of goals is a mixture of Poisson distributions with different values of <c>lam</c>.
    </p>

    <p>
      First I'll generate a sequence of <c>Pmf</c> objects, one for each value of <c>lam</c>.
    </p>

    <program language="python">
      <input>
pmf_seq = [make_poisson_pmf(lam, goals)
           for lam in prior.qs]
      </input>
    </program>

    <p>
      The following figure shows what these distributions look like for a few values of <c>lam</c>.
    </p>

    <p>
      The predictive distribution is a mixture of these <c>Pmf</c> objects, weighted with the posterior probabilities.
      We can use <c>make_mixture</c> from <xref ref="sec-min-max-mixture-mixtures"/> to compute this mixture.
    </p>

    <program language="python">
      <input>
from utils import make_mixture

pred_france = make_mixture(france, pmf_seq)
pred_croatia = make_mixture(croatia, pmf_seq)
      </input>
    </program>

    <p>
      Here's the predictive distribution for the number of goals France would score in a rematch.
      This distribution represents two sources of uncertainty: we don't know the actual value of <c>lam</c>, and even if we did, we would not know the number of goals in the next game.
    </p>

    <p>
      We can use these distributions to compute the probability that France wins, loses, or ties the rematch.
    </p>

    <program language="python">
      <input>
win = Pmf.prob_gt(pred_france, pred_croatia)
lose = Pmf.prob_lt(pred_france, pred_croatia)
tie = Pmf.prob_eq(pred_france, pred_croatia)
      </input>
    </program>

    <p>
      Assuming that France wins half of the ties, their chance of winning the rematch is about 65%.
    </p>

    <p>
      This is a bit lower than their probability of superiority, which is 75%.  And that makes sense, because we are less certain about the outcome of a single game than we are about the goal-scoring rates.
      Even if France is the better team, they might lose the game.
    </p>
  </section>

  <section xml:id="sec-exponential-distribution">
    <title>The Exponential Distribution</title>

    <p>
      As an exercise at the end of this chapter, you'll have a chance to work on the following variation on the World Cup Problem:
    </p>

    <blockquote>
      <p>
        In the 2014 FIFA World Cup, Germany played Brazil in a semifinal match. Germany scored after 11 minutes and again at the 23 minute mark.
        At that point in the match, how many goals would you expect Germany to score after 90 minutes?
        What was the probability that they would score 5 more goals (as, in fact, they did)?
      </p>
    </blockquote>

    <p>
      In this version, notice that the data is not the number of goals in a fixed period of time, but the time between goals.
    </p>

    <p>
      To compute the likelihood of data like this, we can take advantage of the theory of Poisson processes again.  If each team has a constant goal-scoring rate, we expect the time between goals to follow an <url href="https://en.wikipedia.org/wiki/Exponential_distribution">exponential distribution</url>.
    </p>

    <p>
      If the goal-scoring rate is <m>\lambda</m>, the probability of seeing an interval between goals of <m>t</m> is proportional to the PDF of the exponential distribution:
    </p>

    <me>
      \lambda \exp(-\lambda t)
    </me>

    <p>
      Because <m>t</m> is a continuous quantity, the value of this expression is not a probability; it is a probability density.  However, it is proportional to the probability of the data, so we can use it as a likelihood in a Bayesian update.
    </p>

    <p>
      SciPy provides <c>expon</c>, which creates an object that represents an exponential distribution.
      However, it does not take <c>lam</c> as a parameter in the way you might expect, which makes it awkward to work with.
      Since the PDF of the exponential distribution is so easy to evaluate, I'll use my own function.
    </p>

    <program language="python">
      <input>
def expo_pdf(t, lam):
    """Compute the PDF of the exponential distribution."""
    return lam * np.exp(-lam * t)
      </input>
    </program>

    <p>
      To see what the exponential distribution looks like, let's assume again that <c>lam</c> is 1.4; we can compute the distribution of <m>t</m> like this:
    </p>

    <program language="python">
      <input>
lam = 1.4
qs = np.linspace(0, 4, 101)
ps = expo_pdf(qs, lam)
pmf_time = Pmf(ps, qs)
pmf_time.normalize()
      </input>
    </program>

    <p>
      And here's what it looks like.
    </p>

    <p>
      <!-- Figure fig07-05: An exponential distribution with lambda = 1.4. -->
      It is counterintuitive, but true, that the most likely time to score a goal is immediately.  After that, the probability of each successive interval is a little lower.
    </p>

    <p>
      With a goal-scoring rate of 1.4, it is possible that a team will take more than one game to score a goal, but it is unlikely that they will take more than two games.
    </p>
  </section>

  <section xml:id="sec-poisson-summary">
    <title>Summary</title>

    <p>
      This chapter introduces three new distributions, so it can be hard to keep them straight.
      Let's review:
    </p>

    <ul>
      <li><p>If a system satisfies the assumptions of a Poisson model, the number of events in a period of time follows a Poisson distribution, which is a discrete distribution with integer quantities from 0 to infinity.  In practice, we can usually ignore low-probability quantities above a finite limit.</p></li>
      <li><p>Also under the Poisson model, the interval between events follows an exponential distribution, which is a continuous distribution with quantities from 0 to infinity.  Because it is continuous, it is described by a probability density function (PDF) rather than a probability mass function (PMF).  But when we use an exponential distribution to compute the likelihood of the data, we can treat densities as unnormalized probabilities.</p></li>
      <li><p>The Poisson and exponential distributions are parameterized by an event rate, denoted <m>\lambda</m> or <c>lam</c>.</p></li>
      <li><p>For the prior distribution of <m>\lambda</m>, I used a gamma distribution, which is a continuous distribution with quantities from 0 to infinity, but I approximated it with a discrete, bounded PMF.  The gamma distribution has one parameter, denoted <m>\alpha</m> or <c>alpha</c>, which is also its mean.</p></li>
    </ul>

    <p>
      I chose the gamma distribution because the shape is consistent with our background knowledge about goal-scoring rates.
      There are other distributions we could have used; however, we will see in <xref ref="ch-conjugate-priors"/> that the gamma distribution can be a particularly good choice.
    </p>

    <p>
      But we have a few things to do before we get there, starting with these exercises.
    </p>
  </section>

  <exercises xml:id="exercises-poisson">
    <title>Exercises</title>

    <p>
      The code for this chapter is in <c>chap08.ipynb</c>, which is in the repository for this book.  See Section 0.3 for details.
      You can run the notebook on Colab at <url href="https://colab.research.google.com/github/AllenDowney/ThinkBayes2/blob/master/code/chap08.ipynb">https://colab.research.google.com/github/AllenDowney/ThinkBayes2/blob/master/code/chap08.ipynb</url>.
    </p>

    <p>
      The notebook provides space where you can work on the following problems.
    </p>

    <exercise xml:id="ex-poisson-germany-brazil">
      <statement>
        <p>
          Let's finish the exercise we started:
        </p>

        <blockquote>
          <p>
            In the 2014 FIFA World Cup, Germany played Brazil in a semifinal match. Germany scored after 11 minutes and again at the 23 minute mark.
            At that point in the match, how many goals would you expect Germany to score after 90 minutes?
            What was the probability that they would score 5 more goals (as, in fact, they did)?
          </p>
        </blockquote>

        <p>
          Here are the steps I recommend:
        </p>

        <ol>
          <li><p>Starting with the same gamma prior we used in the previous problem, compute the likelihood of scoring a goal after 11 minutes for each possible value of <c>lam</c>.  Don't forget to convert all times into games rather than minutes.</p></li>
          <li><p>Compute the posterior distribution of <c>lam</c> for Germany after the first goal.</p></li>
          <li><p>Compute the likelihood of scoring another goal after 12 more minutes and do another update.  Plot the prior, posterior after one goal, and posterior after two goals.</p></li>
          <li><p>Compute the posterior predictive distribution of goals Germany might score during the remaining time in the game, <c>90-23</c> minutes.  Note: You will have to think about how to generate predicted goals for a fraction of a game.</p></li>
          <li><p>Compute the probability of scoring 5 or more goals during the remaining time.</p></li>
        </ol>
      </statement>
    </exercise>

    <exercise xml:id="ex-poisson-france-scores-first">
      <statement>
        <p>
          Returning to the first version of the World Cup Problem.  Suppose France and Croatia play a rematch.  What is the probability that France scores first?
        </p>

        <p>
          Hint: Compute the posterior predictive distribution for the time until the first goal by making a mixture of exponential distributions.  You can use the following function to make a PMF that approximates an exponential distribution.
        </p>

        <program language="python">
          <input>
def make_expo_pmf(lam, high):
    """Make a PMF of an exponential distribution.

    lam: event rate
    high: upper bound on the interval `t`

    returns: Pmf of the interval between events
    """
    qs = np.linspace(0, high, 101)
    ps = expo_pdf(qs, lam)
    pmf = Pmf(ps, qs)
    pmf.normalize()
    return pmf
          </input>
        </program>
      </statement>
    </exercise>

    <exercise xml:id="ex-poisson-hockey">
      <statement>
        <p>
          In the 2010-11 National Hockey League (NHL) Finals, my beloved Boston Bruins played a best-of-seven championship series against the despised Vancouver Canucks.  Boston lost the first two games 0-1 and 2-3, then won the next two games 8-1 and 4-0.  At this point in the series, what is the probability that Boston will win the next game, and what is their probability of winning the championship?
        </p>

        <p>
          To choose a prior distribution, I got some statistics from <url href="http://www.nhl.com">http://www.nhl.com</url>, specifically the average goals per game for each team in the 2010-11 season.  The distribution is well modeled by a gamma distribution with mean 2.8.
        </p>

        <p>
          In what ways do you think the outcome of these games might violate the assumptions of the Poisson model?  How would these violations affect your predictions?
        </p>
      </statement>
    </exercise>
  </exercises>
</chapter>
